{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91714,"databundleVersionId":11251744,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T05:49:14.090674Z","iopub.execute_input":"2025-03-11T05:49:14.091058Z","iopub.status.idle":"2025-03-11T05:49:15.265838Z","shell.execute_reply.started":"2025-03-11T05:49:14.091017Z","shell.execute_reply":"2025-03-11T05:49:15.264652Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e3/sample_submission.csv\n/kaggle/input/playground-series-s5e3/train.csv\n/kaggle/input/playground-series-s5e3/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import warnings\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\ntrain_data_rt = \"/kaggle/input/playground-series-s5e3/train.csv\"\ntest_data_rt = \"/kaggle/input/playground-series-s5e3/test.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T05:49:15.266885Z","iopub.execute_input":"2025-03-11T05:49:15.267391Z","iopub.status.idle":"2025-03-11T05:49:15.272753Z","shell.execute_reply.started":"2025-03-11T05:49:15.267360Z","shell.execute_reply":"2025-03-11T05:49:15.271189Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\n# Initializing The model\n\nclass RainfallClassifierModel(nn.Module):\n    def __init__(self):\n        super(RainfallClassifierModel, self).__init__()\n        self.base_layers = nn.Sequential(\n            nn.Linear(54, 256),\n            nn.ELU(),\n            nn.BatchNorm1d(256),\n            # nn.Dropout(0.2),  # 20% dropout\n    \n            nn.Linear(256, 512),\n            nn.ELU(),\n            nn.BatchNorm1d(512),\n            # nn.Dropout(0.3),\n\n            nn.Linear(512, 512),\n            nn.ELU(),\n            nn.BatchNorm1d(512),\n            # nn.Dropout(0.3),\n\n            nn.Linear(512, 256),\n            nn.ELU(),\n            nn.BatchNorm1d(256),\n            # nn.Dropout(0.2),\n\n            nn.Linear(256, 1)\n)\n        self.flatten = nn.Flatten()\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.base_layers(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T05:49:15.274000Z","iopub.execute_input":"2025-03-11T05:49:15.274489Z","iopub.status.idle":"2025-03-11T05:49:19.426938Z","shell.execute_reply.started":"2025-03-11T05:49:15.274449Z","shell.execute_reply":"2025-03-11T05:49:19.425693Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Definfing the feature engineering process\n\ndef feature_engineering(df):\n    \"\"\"\n    Create new features based on meteorological understanding and data analysis,\n    with 'day' representing day of the year (1-365).\n    Ensures no data leakage by avoiding use of the target variable (rainfall).\n    \"\"\"\n    # Make a copy to avoid modifying the original dataframe\n    enhanced_df = df.copy()\n    \n    # 1. temparature range (difference between max and min temparatures)\n    enhanced_df['temp_range'] = enhanced_df['maxtemp'] - enhanced_df['mintemp']\n    \n    # 2. Dew point depression (difference between temparature and dew point)\n    enhanced_df['dewpoint_depression'] = enhanced_df['temparature'] - enhanced_df['dewpoint']\n    \n    # 3. Pressure change from previous day\n    enhanced_df['pressure_change'] = enhanced_df['pressure'].diff().fillna(0)\n    \n    # 4. Humidity to dew point ratio\n    enhanced_df['humidity_dewpoint_ratio'] = enhanced_df['humidity'] / enhanced_df['dewpoint'].clip(lower=0.1)\n    \n    # 5. Cloud coverage to sunshine ratio (inverse relationship)\n    enhanced_df['cloud_sunshine_ratio'] = enhanced_df['cloud'] / enhanced_df['sunshine'].clip(lower=0.1)\n    \n    # 6. Wind intensity factor (combination of speed and humidity)\n    enhanced_df['wind_humidity_factor'] = enhanced_df['windspeed'] * (enhanced_df['humidity'] / 100)\n    \n    # 7. temparature-humidity index (simple version of heat index)\n    enhanced_df['temp_humidity_index'] = (0.8 * enhanced_df['temparature']) + \\\n                                        ((enhanced_df['humidity'] / 100) * \\\n                                        (enhanced_df['temparature'] - 14.3)) + 46.4\n    \n    # 8. Pressure change rate (acceleration)\n    enhanced_df['pressure_acceleration'] = enhanced_df['pressure_change'].diff().fillna(0)\n    \n    # 9. Seasonal features (based on day of year)\n    # Convert day to month (1-365 to 1-12)\n    enhanced_df['month'] = ((enhanced_df['day'] - 1) // 30) + 1\n    enhanced_df['month'] = enhanced_df['month'].clip(upper=12)  # Ensure month doesn't exceed 12\n    \n    # 10. Convert day to season (1-365 to 1-4)\n    enhanced_df['season'] = ((enhanced_df['month'] - 1) // 3) + 1\n    \n    # 11. Sine and cosine transformations to capture cyclical nature of days in a year\n    enhanced_df['day_of_year_sin'] = np.sin(2 * np.pi * enhanced_df['day'] / 365)\n    enhanced_df['day_of_year_cos'] = np.cos(2 * np.pi * enhanced_df['day'] / 365)\n    \n    # 12. Rolling averages for key meteorological variables\n    for window in [3, 7, 14]:\n        enhanced_df[f'temparature_rolling_{window}d'] = enhanced_df['temparature'].rolling(window=window, min_periods=1).mean()\n        enhanced_df[f'pressure_rolling_{window}d'] = enhanced_df['pressure'].rolling(window=window, min_periods=1).mean()\n        enhanced_df[f'humidity_rolling_{window}d'] = enhanced_df['humidity'].rolling(window=window, min_periods=1).mean()\n        enhanced_df[f'cloud_rolling_{window}d'] = enhanced_df['cloud'].rolling(window=window, min_periods=1).mean()\n        enhanced_df[f'windspeed_rolling_{window}d'] = enhanced_df['windspeed'].rolling(window=window, min_periods=1).mean()\n    \n    # 13. Weather pattern change features\n    # temparature trend\n    enhanced_df['temp_trend_3d'] = enhanced_df['temparature'].diff(3).fillna(0)\n    # Pressure trend\n    enhanced_df['pressure_trend_3d'] = enhanced_df['pressure'].diff(3).fillna(0)\n    # Humidity trend\n    enhanced_df['humidity_trend_3d'] = enhanced_df['humidity'].diff(3).fillna(0)\n    \n    # 14. Extreme weather indicators\n    enhanced_df['extreme_temp'] = (enhanced_df['temparature'] > enhanced_df['temparature'].quantile(0.95)) | \\\n                                 (enhanced_df['temparature'] < enhanced_df['temparature'].quantile(0.05))\n    enhanced_df['extreme_temp'] = enhanced_df['extreme_temp'].astype(int)\n    \n    enhanced_df['extreme_humidity'] = (enhanced_df['humidity'] > enhanced_df['humidity'].quantile(0.95)) | \\\n                                     (enhanced_df['humidity'] < enhanced_df['humidity'].quantile(0.05))\n    enhanced_df['extreme_humidity'] = enhanced_df['extreme_humidity'].astype(int)\n    \n    enhanced_df['extreme_pressure'] = (enhanced_df['pressure'] > enhanced_df['pressure'].quantile(0.95)) | \\\n                                     (enhanced_df['pressure'] < enhanced_df['pressure'].quantile(0.05))\n    enhanced_df['extreme_pressure'] = enhanced_df['extreme_pressure'].astype(int)\n    \n    # 15. Interaction terms between key variables\n    enhanced_df['temp_humidity_interaction'] = enhanced_df['temparature'] * enhanced_df['humidity']\n    enhanced_df['pressure_wind_interaction'] = enhanced_df['pressure'] * enhanced_df['windspeed']\n    enhanced_df['cloud_sunshine_interaction'] = enhanced_df['cloud'] * enhanced_df['sunshine']\n    enhanced_df['dewpoint_humidity_interaction'] = enhanced_df['dewpoint'] * enhanced_df['humidity']\n    \n    # 16. Moving standard deviations for measuring variability\n    for window in [7, 14]:\n        enhanced_df[f'temp_std_{window}d'] = enhanced_df['temparature'].rolling(window=window, min_periods=4).std().fillna(0)\n        enhanced_df[f'pressure_std_{window}d'] = enhanced_df['pressure'].rolling(window=window, min_periods=4).std().fillna(0)\n        enhanced_df[f'humidity_std_{window}d'] = enhanced_df['humidity'].rolling(window=window, min_periods=4).std().fillna(0)\n    \n    return enhanced_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T05:49:19.429180Z","iopub.execute_input":"2025-03-11T05:49:19.429761Z","iopub.status.idle":"2025-03-11T05:49:19.446270Z","shell.execute_reply.started":"2025-03-11T05:49:19.429722Z","shell.execute_reply":"2025-03-11T05:49:19.444472Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_data = pd.read_csv(train_data_rt)\ntest_data = pd.read_csv(test_data_rt)\nindices = test_data['id']\ntest_data = test_data.drop(columns = [\"id\"])\n\ntrain_data_fe = feature_engineering(train_data)\ntest_data_fe = feature_engineering(test_data).to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T05:49:19.448166Z","iopub.execute_input":"2025-03-11T05:49:19.448634Z","iopub.status.idle":"2025-03-11T05:49:19.613711Z","shell.execute_reply.started":"2025-03-11T05:49:19.448588Z","shell.execute_reply":"2025-03-11T05:49:19.612796Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Creating the custom dataloader for loading the data\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CSVDataset(Dataset):\n    def __init__(self, df):\n        self.data = df.drop(columns = [\"id\", \"rainfall\"]).values # In order to load them as a numpy array\n        self.target = df[\"rainfall\"].values\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        features = torch.tensor(self.data[idx, :], dtype = torch.float32)\n        labels = torch.tensor(self.target[idx], dtype = torch.float32)\n\n        return features, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T05:49:19.614491Z","iopub.execute_input":"2025-03-11T05:49:19.614846Z","iopub.status.idle":"2025-03-11T05:49:19.623402Z","shell.execute_reply.started":"2025-03-11T05:49:19.614815Z","shell.execute_reply":"2025-03-11T05:49:19.622030Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Initializing the model and other functionalities\nimport torch.optim as optim\n\nmodel = RainfallClassifierModel()\noptimizer = optim.SGD(model.parameters(), lr = 0.0001)\ncriterion = nn.BCEWithLogitsLoss()\n\ndataset = CSVDataset(train_data_fe)\ndataloader = DataLoader(dataset, batch_size = 32, shuffle = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T05:49:19.625336Z","iopub.execute_input":"2025-03-11T05:49:19.626113Z","iopub.status.idle":"2025-03-11T05:49:22.218386Z","shell.execute_reply.started":"2025-03-11T05:49:19.626045Z","shell.execute_reply":"2025-03-11T05:49:22.217327Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Function to initialize weights\n\nimport torch.nn.init as init\n\ndef initialize_weights(model, init_type=\"xavier\"):\n    for m in model.modules():\n        if isinstance(m, nn.Linear):  # Apply to linear layers\n            if init_type == \"xavier\":\n                init.xavier_uniform_(m.weight)  # Xavier initialization\n            elif init_type == \"he\":\n                init.kaiming_uniform_(m.weight, nonlinearity='relu')  # He initialization\n            elif init_type == \"orthogonal\":\n                init.orthogonal_(m.weight)  # Orthogonal initialization\n            else:\n                raise ValueError(\"Unknown initialization type\")\n            if m.bias is not None:\n                init.zeros_(m.bias)  # Initialize bias to zero\n\ninitialize_weights(model, init_type = \"he\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T05:49:22.219436Z","iopub.execute_input":"2025-03-11T05:49:22.220030Z","iopub.status.idle":"2025-03-11T05:49:22.233712Z","shell.execute_reply.started":"2025-03-11T05:49:22.219987Z","shell.execute_reply":"2025-03-11T05:49:22.232329Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Training Code\n\nnum_of_epochs = 50\n\nfor i in range(num_of_epochs):\n    model.train()\n    for batch_x, batch_y in dataloader:\n        optimizer.zero_grad()\n        y_pred = model(batch_x).squeeze(1)\n        \n        loss = criterion(y_pred, batch_y.float())  # BCE loss\n        \n        loss.backward()\n        optimizer.step()\n    print(f\"Epoch : {i} --------- Loss : {loss}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T05:49:22.234956Z","iopub.execute_input":"2025-03-11T05:49:22.235365Z","iopub.status.idle":"2025-03-11T05:49:37.818059Z","shell.execute_reply.started":"2025-03-11T05:49:22.235321Z","shell.execute_reply":"2025-03-11T05:49:37.817008Z"}},"outputs":[{"name":"stdout","text":"Epoch : 0 --------- Loss : 0.7921128273010254\nEpoch : 1 --------- Loss : 0.7161997556686401\nEpoch : 2 --------- Loss : 0.7536503076553345\nEpoch : 3 --------- Loss : 0.5513197183609009\nEpoch : 4 --------- Loss : 0.5750579237937927\nEpoch : 5 --------- Loss : 0.5257330536842346\nEpoch : 6 --------- Loss : 0.6927876472473145\nEpoch : 7 --------- Loss : 0.5566142797470093\nEpoch : 8 --------- Loss : 0.7003359794616699\nEpoch : 9 --------- Loss : 0.7775865793228149\nEpoch : 10 --------- Loss : 0.5804604291915894\nEpoch : 11 --------- Loss : 0.3874354362487793\nEpoch : 12 --------- Loss : 0.6456301808357239\nEpoch : 13 --------- Loss : 0.6356551051139832\nEpoch : 14 --------- Loss : 0.43073710799217224\nEpoch : 15 --------- Loss : 0.8247550129890442\nEpoch : 16 --------- Loss : 0.6664888262748718\nEpoch : 17 --------- Loss : 0.6330674886703491\nEpoch : 18 --------- Loss : 0.539782702922821\nEpoch : 19 --------- Loss : 0.5329039096832275\nEpoch : 20 --------- Loss : 0.6285162568092346\nEpoch : 21 --------- Loss : 0.6558502316474915\nEpoch : 22 --------- Loss : 0.5345392227172852\nEpoch : 23 --------- Loss : 0.5066478848457336\nEpoch : 24 --------- Loss : 0.5143027901649475\nEpoch : 25 --------- Loss : 0.4597316086292267\nEpoch : 26 --------- Loss : 0.482190877199173\nEpoch : 27 --------- Loss : 0.5002909302711487\nEpoch : 28 --------- Loss : 0.5083335041999817\nEpoch : 29 --------- Loss : 0.5774084329605103\nEpoch : 30 --------- Loss : 0.4602540135383606\nEpoch : 31 --------- Loss : 0.5128260254859924\nEpoch : 32 --------- Loss : 0.5788305997848511\nEpoch : 33 --------- Loss : 0.7915687561035156\nEpoch : 34 --------- Loss : 0.6255308389663696\nEpoch : 35 --------- Loss : 0.5719631314277649\nEpoch : 36 --------- Loss : 0.6330607533454895\nEpoch : 37 --------- Loss : 0.5600231289863586\nEpoch : 38 --------- Loss : 0.6716833114624023\nEpoch : 39 --------- Loss : 0.4886940121650696\nEpoch : 40 --------- Loss : 0.7031715512275696\nEpoch : 41 --------- Loss : 0.5769746899604797\nEpoch : 42 --------- Loss : 0.7190284132957458\nEpoch : 43 --------- Loss : 0.6017056107521057\nEpoch : 44 --------- Loss : 0.594489574432373\nEpoch : 45 --------- Loss : 0.4558372497558594\nEpoch : 46 --------- Loss : 0.49964162707328796\nEpoch : 47 --------- Loss : 0.684963583946228\nEpoch : 48 --------- Loss : 0.44550296664237976\nEpoch : 49 --------- Loss : 0.5001168847084045\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Prediction code\n\nX_test_tensor = torch.tensor(test_data_fe, dtype=torch.float32)\n\nmodel.eval()\n\nwith torch.no_grad():  # No gradients needed for inference\n    logits = model(X_test_tensor)  # Forward pass\n    probabilities = torch.sigmoid(logits)  # Apply Sigmoid (for binary classification)\n\nprobabilities = torch.nan_to_num(probabilities, nan=0.5)\n\nprint(torch.isnan(probabilities).sum())  # Count NaN values\nprint(torch.isinf(probabilities).sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T05:49:37.819044Z","iopub.execute_input":"2025-03-11T05:49:37.819321Z","iopub.status.idle":"2025-03-11T05:49:37.848163Z","shell.execute_reply.started":"2025-03-11T05:49:37.819296Z","shell.execute_reply":"2025-03-11T05:49:37.847066Z"}},"outputs":[{"name":"stdout","text":"tensor(0)\ntensor(0)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Concatenation\n\nfinal_dataframe = np.concatenate((np.array(indices).reshape(-1, 1), probabilities.numpy()), axis = 1)\nsubmission = pd.DataFrame(final_dataframe, columns = [\"id\", \"rainfall\"])\nsubmission[\"id\"] = submission[\"id\"].astype(int)\nsubmission.to_csv(\"submission.csv\", index = False)\n\nsubmission.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T05:49:37.849251Z","iopub.execute_input":"2025-03-11T05:49:37.849520Z","iopub.status.idle":"2025-03-11T05:49:37.870457Z","shell.execute_reply.started":"2025-03-11T05:49:37.849494Z","shell.execute_reply":"2025-03-11T05:49:37.869329Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"id          0\nrainfall    0\ndtype: int64"},"metadata":{}}],"execution_count":11}]}